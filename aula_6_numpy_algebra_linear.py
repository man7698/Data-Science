# -*- coding: utf-8 -*-
"""Aula_6_Numpy_Algebra_Linear.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/storopoli/ciencia-de-dados/blob/master/notebooks/Aula_6_Numpy_Algebra_Linear.ipynb

[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/storopoli/ciencia-de-dados/master?filepath=notebooks%2FAula_6_Numpy_Algebra_Linear.ipynb)
<br>
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/storopoli/ciencia-de-dados/blob/master/notebooks/Aula_6_Numpy_Algebra_Linear.ipynb)

# Algebra Linear e `NumPy`

**Objetivos**: Rever conceitos de Álgebra Linear e apresentar a biblioteca `NumPy`.

Recomendo assistirem a série *Essence of Linear Algebra* do canal do YouTube [3blue1brown](https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw)



[<img src="https://i.ytimg.com/vi/kjBOesZCoqc/maxresdefault.jpg" alt="Essence of Linear Algebra" style="width: 300px;"/>](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)

## Notação

Matriz $\mathbf{A} \in \mathbb{R}^{m \times n}$

Vetor $\vec{v} \in \mathbb{R}^{n}$

## Matriz

Matriz $\mathbf{A} \in \mathbb{R}^{2 \times 3}$

$$\mathbf{A} = \left[\begin{array}{lll}{1} & {2} & {3} \\ {4} & {5} & {6}\end{array}\right]$$

## Vetor
Vetor $\vec{v} \in \mathbb{R}^{3}$

$$\vec{v} = \left[\begin{array}{l}{1} \\ {2} \\ {3}\end{array}\right]$$
"""

import numpy as np
A = np.array([[1,2,3],
              [4,5,6]])
v = np.array([1,
              2,
              3])
print(A)
print(v)

"""## Multiplicação de Matrizes

O produto $\mathbf{AB}$ das matrizes $\mathbf{A} \in \mathbb{R}^{m \times \ell}$ e $\mathbf{B} \in \mathbb{R}^{\ell \times n}$ consiste em computar o produto entre cada linha da $\mathbf{A}$ com cada coluna de $\mathbf{B}$:

$$\mathbf{C}=\mathbf{A} \mathbf{B} \quad \Leftrightarrow \quad c_{i j}=\sum_{k=1}^{\ell} a_{i k} b_{k j}, \forall i \in[1, \ldots, m], j \in[1, \ldots, n].$$

$$\left[\begin{array}{ll}{a_{11}} & {a_{12}} \\ {a_{21}} & {a_{22}} \\ {a_{31}} & {a_{32}}\end{array}\right]\left[\begin{array}{ll}{b_{11}} & {b_{12}} \\ {b_{21}} & {b_{22}}\end{array}\right]=\left[\begin{array}{ll}{a_{11} b_{11}+a_{12} b_{21}} & {a_{11} b_{12}+a_{12} b_{22}} \\ {a_{21} b_{11}+a_{22} b_{21}} & {a_{21} b_{12}+a_{22} b_{22}} \\ {a_{31} b_{11}+a_{32} b_{21}} & {a_{31} b_{12}+a_{32} b_{22}}\end{array}\right] \in \mathbb{R}^{3 \times 2}.$$

![matrix-multiplication](https://f0.pngfuel.com/png/267/924/matrix-multiplication-field-row-the-matrix-png-clip-art.png)

### Multiplicação de Vetor por Matriz

$$\textbf{A} \vec{x}=\left[\begin{array}{ccc}
1 & 4 & 5 \\
3 & 2 & 5 \\
2 & 1 & 3
\end{array}\right]\left[\begin{array}{c}
x_{1} \\
x_{2} \\
x_{3}
\end{array}\right]=\left[\begin{array}{c}
1 \\
3 \\
2
\end{array}\right] x_{1}+\left[\begin{array}{c}
4 \\
2 \\
1
\end{array}\right] x_{2}+\left[\begin{array}{c}
5 \\
5 \\
3
\end{array}\right] x_{3}$$
"""

A = np.array([[3,0],
              [0,5],
              [1,5]])
B = np.array([[3,0],
              [0,5]])

np.dot(A,B)

"""## *Digressão* - Por que Matrizes são Importantes?

### Imagens e Filtros

Todo dado estruturado é representado como matrizes no computador. Veja o caso de imagens. Elas são literalmente matrizes multidimensionais.

* Uma imagem colorida é uma Array de dimensão $X \times Y \times 3$:
    * $X$ = quantidade de pixels no eixo horizontal
    * $Y$ = quantidade de pixels no eixo vertical
    * $3$ = são 3 canais de cores - **R**ed, **G**reen e **B**lue

<img src="https://github.com/storopoli/ciencia-de-dados/blob/master/notebooks/images/images_as_matrix.png?raw=1" alt="Row vs Cols" style="width: 500px;"/>
"""

import matplotlib.pyplot as plt

feynman = plt.imread("images/feynman.png")
print(feynman.shape)
plt.imshow(feynman, cmap='gray')
plt.title('Quem é essa pessoa?')
plt.show()

"""#### Convoluções

<img src="https://github.com/storopoli/ciencia-de-dados/blob/master/notebooks/images/convolution.gif?raw=1" alt="Row vs Cols" style="width: 400px;"/>
"""

from scipy.signal import convolve

"""#### Filtros

Temos vários. Veja essa [página na Wikipedia](https://en.wikipedia.org/wiki/Kernel_(image_processing)).

##### Detecção de Bordas (*Edge Detection*)

$$
\left[\begin{array}{lll}
{-1} & {-1} & {-1} \\
{-1} & {+8} & {-1} \\
{-1} & {-1} & {-1}
\end{array}\right]
$$
"""

edge_detect = np.array([[-1, -1, -1],
                       [-1, 8, -1],
                       [-1, -1, -1]])

feynman_edge = convolve(feynman, edge_detect)
plt.imshow(feynman_edge, cmap='gray')
plt.title('Edge Detection')
plt.show()

"""Não ficou muto bom né?

Vamos então usar o [Filtro de Sobel](https://en.wikipedia.org/wiki/Sobel_operator) para bordas verticais e horizontais:

$$\mathbf{G}_{\text{vertical}} = \begin{bmatrix} 
 +1 & 0 & -1  \\
+2 & 0 & -2 \\
+1 & 0 & -1 
\end{bmatrix}
\quad
\mbox{e}
\quad   
\mathbf{G}_{\text{horizontal}} = \begin{bmatrix} 
 +1 & +2 & +1\\
 0 & 0 & 0 \\
-1 & -2 & -1
\end{bmatrix}$$
"""

sobel_vertical = np.array([[1, 0, -1],
                           [2, 0, -2],
                           [1, 0, -1]])

sobel_horizontal = np.array([[1, 2, 1],
                             [0, 0, 0],
                             [-1, -2, -1]])

feynman_sobel_vert = convolve(feynman, sobel_vertical)
feynman_sobel_horiz = convolve(feynman, sobel_horizontal)

fig, ax = plt.subplots(1, 2, figsize=(10, 5))
fig.tight_layout()
ax[0].imshow(feynman_sobel_vert, cmap='gray')
ax[0].title.set_text('Vertical')
ax[1].imshow(feynman_sobel_horiz, cmap='gray')
ax[1].title.set_text('Horizontal')
plt.show()

"""##### Aumento de Nitidez (*Sharpen*)

$$
\left[\begin{array}{lll}
{0} & {-1} & {0} \\
{-1} & {+5} & {-1} \\
{0} & {-1} & {0}
\end{array}\right]
$$
"""

sharpen = np.array([[0, -1, 0],
                    [-1, 5, -1],
                    [0, -1, 0]])

feynman_sharpen = convolve(feynman, sharpen)
plt.imshow(feynman_sharpen, cmap='gray')
plt.title('Sharpen')
plt.show()

"""##### Desfoque (*Blur*)

$$
\frac{1}{16}
\left[\begin{array}{lll}
1 & 2 & 1 \\
2 & 4 & 2 \\
1 & 2 & 1
\end{array}\right]
$$
"""

blur = np.array([[1, 2, 1],
                 [2, 4, 2],
                 [1, 2, 1]]) / 16

feynman_blur = convolve(feynman, blur)
plt.imshow(feynman_blur, cmap='gray')
plt.title('Blur')
plt.show()

"""Não ficou muito bom né?

Vamos tentar aumentar o número de passadas do filtro (kernel)
"""

feynman_blur_rep = convolve( # 1
    convolve( # 2
        convolve( # 3
            convolve( # 4
                convolve( # 5
                    convolve(feynman, blur), # 6
                    blur), # 5
                blur), # 4
            blur), # 3
        blur), # 2
    blur) # 1
plt.imshow(feynman_blur_rep, cmap='gray')
plt.title('6x Blur')
plt.show()

"""Inclusive, convoluções, onde a "máquina" aprende os filtros (os números que vão dentro da matriz), é a base das redes neurais que detectam objetos!

<img src="https://github.com/storopoli/ciencia-de-dados/blob/master/notebooks/images/deeplearning_convolutions.gif?raw=1" alt="Deep Learning Convolutions" style="width: 600px;"/>

## *Digressão 2* - Por que Matrizes são Importantes?

### Regressão

Todo dado estruturado é representado como matrizes no computador. Veja o caso de dados de uma pesquisa:

* $\mathbf{X} \in \mathbb{R}^{m, n}$ = Dados, onde $m$ é o número de linhas (observações) e $n$ o número de colunas (variáveis)
* $\mathbf{y}$ = vetor (variável) que estamos interessados

$$
\mathbf{X} =
\left[\begin{array}{lll}
1.5 & 2.3 \\
2.5 & 4.2 \\
0.4 & 1.2 \\
3.2 & 3.1 \\
\end{array}\right] \quad \text{e} \quad
\mathbf{y} =
\left[\begin{array}{lll}
2.7 \\
3.4 \\
5.2 \\
6.7 \\
\end{array}\right]
$$

Eu quero achar a relação entre as colunas (variáveis) de $\mathbf{X}$ e a resposta $\mathbf{y}$. Particularmente quero achar o tamanho do efeito $(\beta)$ de cada variável de $\mathbf{X}$ em $\mathbf{y}$:

$$\mathbf{X} \cdot \beta = \mathbf{y}$$

Mas temos um sistema de equações com $m = 4$ equações e 2 variáveis $\beta = 2$, ou seja, insolucionável:

$$
\begin{aligned}
1.5\beta_1 &+ 2.3\beta_2 &= 2.7\\
2.5\beta_1 &+ 4.2\beta_2 &= 3.4\\
0.4\beta_1 &+ 1.2\beta_2 &= 5.2\\
3.2\beta_1 &+ 3.1\beta_2 &= 6.7
\end{aligned}
$$

Solução:

Multiplicar os dois lados por $\mathbf{X}^T$:

$$\mathbf{X}^T \mathbf{X} \beta = \mathbf{X}^T \mathbf{y}$$

Rearranjando as coisas:

$$\beta = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y} $$
"""

X = np.array([[1.5, 2.3],
              [2.5, 4.2],
              [0.4, 1.2],
              [3.2, 3.1]])
y = np.array([2.7, 3.4, 5.2, 6.7])

# (X.T * X)^1 = np.linalg.inv(X.T.dot(X))
beta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)

print(beta)

"""Pronto! Eu sei que a primeira variável de $\mathbf{X}$ impacta positivamente a resposta $\mathbf{y}$ em $\approx 1.4$ e a segunda variável de $\mathbf{X}$ positivamente em $\approx 0.4$!

## *Digressão 3* - Por que Matrizes são Importantes?

### Redução de Dimensão com *Single Value Decomposition* -- SVD

Eu tenho uma matrix $\mathbf{A}$ que pode ser decomposta em:

$$
\mathbf{A}^{m \times n} = \mathbf{U}^{m \times m} \mathbf{\Sigma}^{m \times n} {\mathbf{V}^{T \space n \times n}}$$

Aonde:

* $\mathbf{U}$: Matriz Ortogonal $\mathbb{R}^{m \times m}$
* $\mathbf{V}^T$: Matriz Ortogonal $\mathbb{R}^{n \times n}$
* $\mathbf{\Sigma}$: Matriz de Valores Singulares, $\Sigma_{ii} = \sigma_i \geq 0, \Sigma_{ij} = 0, i \neq j$

Os números da diagonal de $\mathbf{\Sigma}$ são os **valores singulares**:

* $u_i$ são os vetores de valores singulares à esquerda
* $v_i$ são os vetores de valores singulares à direita

#### Aplicações

- **Compressão de Imagens**: matriz $\mathbf{A}$ é a soma de matrizes com rank-1 $\mathbf{A}^* = \sigma_1 u_1 v^T_1 + \dots + \sigma_n u_n v^T_n$. Pegue a submatriz $\mathbf{A}^*_n$ de $\mathbf{A}$ para obter a compressão desejada.
- **Filtragem Colaborativa**: $\mathbf{U}$ é um vetor de variáveis de usuários. $\mathbf{V}$ é um vetor de variáveis dos items (vídeos, músicas, compras etc.). Usuário $i$ gosta do item $j$: multiplicação de $u_i$ com $v_j$.

#### Compressão de Imagens
"""

U, sigma, VT = np.linalg.svd(feynman)

print(f"Dimensões imagem: {feynman.shape}")
print(f"Dimensões U: {U.shape}")
print(f"Dimensões sigma: {sigma.shape}")
print(f"Dimensões VT: {VT.shape}")

"""##### Reconstrução"""

feynman_reconstruido = np.matrix(U[:, :450]) * np.diag(sigma) * np.matrix(VT)
plt.imshow(feynman_reconstruido, cmap='gray')
plt.show()

def compressao_imagem(x, dim):
    U, sigma, VT = np.linalg.svd(x)
    print(f"Número de Valores Singulares: {dim}")
    print(f"Tamanho das Matrizes - U: {U[:, :dim].shape} - sigma: {sigma[:dim].shape} - VT: {VT[:dim, :].shape}")
    print(f"Total de Compressão (Armazenagem): {round((( (dim * 562) + dim + (dim * 450) ) / (562 * 450)) * 100, 2)}%")
    print("------------------------------------------------------")
    return np.matrix(U[:, :dim]) * np.diag(sigma[:dim]) * np.matrix(VT[:dim, :])

feynman_10 = compressao_imagem(feynman, 10)
feynman_50 = compressao_imagem(feynman, 50)
feynman_100 = compressao_imagem(feynman, 100)
feynman_200 = compressao_imagem(feynman, 200)

fig, ax = plt.subplots(1, 4, figsize=(10, 6))
fig.tight_layout()

ax[0].imshow(feynman_10, cmap='gray')
ax[0].title.set_text('10')

ax[1].imshow(feynman_50, cmap='gray')
ax[1].title.set_text('50')

ax[2].imshow(feynman_100, cmap='gray')
ax[2].title.set_text('100')

ax[3].imshow(feynman_200, cmap='gray')
ax[3].title.set_text('200')

plt.show()

"""## Matriz Transposta

A matriz transposta $\mathbf{A}^T$ é definida pela fórmula $a{^T}_{ij} = a_{ji}$. Em outras palavras, obtemos a transposição por “virar” a matriz pela sua diagonal:

$$^{T}: \mathbb{R}^{m \times n} \rightarrow \mathbb{R}^{n \times m}$$

$$\left[\begin{array}{lll}{\alpha_{1}} & {\alpha_{2}} & {\alpha_{3}} \\ {\beta_{1}} & {\beta_{2}} & {\beta_{3}}\end{array}\right]^{\top}=\left[\begin{array}{ll}{\alpha_{1}} & {\beta_{1}} \\ {\alpha_{2}} & {\beta_{2}} \\ {\alpha_{3}} & {\beta_{3}}\end{array}\right]$$
"""

A = np.array([[1,2,3],
              [4,5,6]])

A.T

"""## Determinante
A determinante de uma matriz é um procedimento que envolve todos os elementos da matriz, e que o resultado é um número único:

$$\operatorname{det}: \mathbb{R}^{n \times n} \rightarrow \mathbb{R}.$$

A determinante descreve a geometria relativa dos vetores que fazem as linhas de uma matriz. Mais especificamente, a determinante de uma matriz $\mathbf{A}$ diz respeito sobre o *volume* de uma caixa com os lados dados pelas linhas de $\mathbf{A}$.

A determinante de uma matriz $2 \times 2$ é:

$$\operatorname{det}(\mathbf{A})=\operatorname{det}\left(\left[\begin{array}{ll}{a} & {b} \\ {c} & {d}\end{array}\right]\right)=\left|\begin{array}{ll}{a} & {b} \\ {c} & {d}\end{array}\right|=a d-b c$$
"""

A = np.array([[1,2],
              [3,4]])
np.linalg.det(A)

"""## Matrizes e Vetores no NumPy

Todas as matrizes e vetores são chamadas de `arrays` no NumPy

### Criando `array`
"""

A = np.array([[1,2,3],
              [4,5,6]])
A

"""$$\left[\begin{array}{lll}{0} & {0} & {0} \\ {0} & {0} & {0}\end{array}\right]$$"""

A = np.zeros((2,3))
A

"""$$\left[\begin{array}{lll}{1} & {1} & {1} \\ {1} & {1} & {1}\end{array}\right]$$"""

A = np.ones((2,3))
A

"""$$I_3 = \left[\begin{array}{lll}{1} & {0} & {0} \\ {0} & {1} & {0} \\ {0} & {0} & {1} \end{array}\right]$$"""

A = np.eye(3)
A

"""#### Intervalar"""

A = np.arange(1, 6, 1)
A

A = np.linspace(0, 2, 9) # 9 números de 0 a 2
A

A = np.arange(6) # 1-D
print('A = ', A)

B = np.arange(12).reshape(4,3) # 2-D
print('B = ', B)

C = np.arange(24).reshape(2,3,4) # 3-D
print('C = ', C)

"""#### Randômicos"""

A = np.random.rand(3, 2) # float entre 0 e 1
A

A = np.random.randint(1, 10, size = (3, 2)) # int entre 1 e 9
A

A = np.random.uniform(1, 10, size = (3, 2)) # float entre 1 e 9
A

A = np.random.normal(0, scale = 1, size = (3,2)) # normal media 0 e dp 1
A

"""### Propriedades de `array`"""

A = np.array([[1,2,3],
              [4,5,6]])

A.ndim

A.shape

A.size

"""### Operações de `array`"""

a = np.array([20,30,40,50])
b = np.array([0, 1, 2, 3])

a - b

b**2

a < 35

"""#### Multiplicações"""

A = np.array([[1,1],
              [0,1]])
B = np.array([[2,0],
              [3,4]])

A * B # multiplicação elemento-por-elemento

A.dot(B) # multiplicação de Matriz

"""#### Operações Matemáticas"""

B = np.array([1,2,3])

print(np.exp(B))
print(np.sqrt(B))
print(np.log(B))

"""### Indexando e fatiando `array`

Arrays 1-D podem ser indexadas, fatiadas e iteradas que nem `list` e outras sequências Python
"""

A = np.array([1,2,3,4,5,6,7,8,9,10])

print(A[2]) # terceiro elemento
print(A[2:5]) # terceiro a quinto
print(A[:6:2]) # primeiro a sexto elemento a cada 2 elementos

"""#### `arrays` Multidimensionais
`arrays` N-D ($N>1$) podem ser indexadas por eixo
"""

A = np.array([[1,2,3],
              [4,5,6],
              [7,8,9]])

print(A[1, 0]) # segunda linha, primeira coluna
print(A[:2, 1]) # primeira à segunda linha, apenas da segunda coluna

"""## Observação

Matrizes e Arrays em `NumPy` são *Row Major* e não *Column Major*. Ou seja a memória é alocada de maneira contígua usando uma lógica de linhas (*rows*) ao invés de colunas (*columns*).

* Geralmente linguagens de computação voltadas para computação científica são *Column Major*: `R`, `Fortran` e `Julia`.

* Geralmente linguagens de computação que tem uma relação muito próxima com `C` ou `C++` como `Python` são *Row Major*.

<img src="https://github.com/storopoli/ciencia-de-dados/blob/master/notebooks/images/row-vs-cols_1.png?raw=1" alt="Row vs Cols" style="width: 300px;"/>

<img src="https://github.com/storopoli/ciencia-de-dados/blob/master/notebooks/images/row-vs-cols_2.png?raw=1" alt="Row vs Cols" style="width: 300px;"/>

Então se você tiver que percorrer uma `np.ndarray` (Array `NumPy`) sempre faça primeiro pela linha (*row*) do que coluna (*column*):
"""

def col_iter(x):
    s = 0
    for i in range(0, x.shape[1]):
        for j in range(0, x.shape[0]):
            s = s + x[j, i] ** 2
            x[j, i] = s

def row_iter(x):
    s = 0
    for i in range(0, x.shape[0]):
        for j in range(0, x.shape[1]):
            s = s + x[i, j] ** 2
            x[i, j] = s

import numpy as np
np.random.seed(123)
A = np.random.rand(1000, 1000)

# Commented out IPython magic to ensure Python compatibility.
# %timeit col_iter(A)

# Commented out IPython magic to ensure Python compatibility.
# %timeit row_iter(A)