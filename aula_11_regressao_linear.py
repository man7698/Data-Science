# -*- coding: utf-8 -*-
"""Aula_11_Regressao_Linear.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/storopoli/ciencia-de-dados/blob/master/notebooks/Aula_11_Regressao_Linear.ipynb

[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/storopoli/ciencia-de-dados/master?filepath=notebooks%2FAula_11_Regressao_Linear.ipynb)
<br>
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/storopoli/ciencia-de-dados/blob/master/notebooks/Aula_11_Regressao_Linear.ipynb)

# Regressão Linear

**Objetivos**: Aprender o que é Regressão Linear e introduzir intuições sobre o Método do Gradiente e o Método do Gradiente Estocástico assim como os problemas de regressão de aprendizagem de máquina. Apresentar a biblioteca `SciKit-Learn`.

## Defininição - Regressão Linear

> Uma regressão linear faz uma predição simplesmente computando uma soma ponderada dos atributos (*features*), mais uma constante chamada viés (*bias*), também chamado de constante (*intercept*).

<img src="https://github.com/storopoli/ciencia-de-dados/blob/master/notebooks/images/reg-linear.png?raw=1" alt="reg-linear" style="width: 400px;"/>

$$ \hat{y} = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \dots + \theta_n x_n$$

$\hat{y}$ - valor previsto

$\theta$ - parâmetro do modelo

$n$ - número de atributos (*features*)

$x_i$ - o valor do *inésimo* atributo (*feature*)

### Exemplo

$$\mathrm{preço~de~residência} = 4500 + 1000\times \mathrm{quartos} + 120 \times \mathrm{m}^2 + 3000 \times \mathrm{banheiros} - 1500 \times \mathrm{distância~do~centro~km}$$

## Métricas de Desempenho de uma Regressão

<img src="https://github.com/storopoli/ciencia-de-dados/blob/master/notebooks/images/erro-reg.png?raw=1" alt="erro-reg" style="width: 400px;"/>

### *Mean Squared Error* (MSE) - Erro Quadrático Médio

$$MSE = \frac{1}{m}\Sigma_{i=1}^{m}{(\hat{y}_i - y_i)^2}$$

### *Mean Absolute Error* (MAE) - Erro Absoluto Médio
$$MAE = \frac{1}{m}\Sigma_{i=1}^{m}{|\hat{y}_i - y_i|}$$

<img src="https://github.com/storopoli/ciencia-de-dados/blob/master/notebooks/images/gradient-descent.gif?raw=1" alt="gradient-descent-animation" style="width: 500px;"/>

<img src="https://github.com/storopoli/ciencia-de-dados/blob/master/notebooks/images/gradient-descent-2.gif?raw=1" alt="gradient-descent-animation" style="width: 500px;"/>

## Exemplo com o dataset [Boston House Prices](https://scikit-learn.org/stable/datasets/toy_dataset.html#boston-house-prices-dataset)


* $N = 506$
* Atributos: 13
    * `CRIM` crime per capita da região
    * `ZN` proporção de terra residencial
    * `INDUS` proporção terra comercial não-varejista
    * `CHAS` *Dummy* se fica as margens do Charles River (1 ou 0)
    * `NOX` concentração de óxido nítrico (partes por 10 milhões)
    * `RM` número de quartos
    * `AGE` idade da residência
    * `DIS` distância dos cinco centros de emprego de Boston
    * `RAD` acessibilidade às rodovias radiais
    * `TAX` valor do IPTU por 10,000 USD
    * `PTRATIO` relação aluno-professor (*pupil-teacher ratio*) da região
    * `B` proporção de afro-descendentes na região
    * `LSTAT` porcentagem de população de baixa-renda
* Variável resposta: valor da casa por 10,000 USD
"""

from sklearn.datasets import load_boston

boston = load_boston()
X = boston['data']
y = boston['target']

print(f"Nomes dos Atributos: {boston['feature_names']}")
print(f"Tamanho de X: {X.shape}")
print(f"Tamanho de y: {y.shape}")

"""### Quebrando dataset em `train` e `test`

Usar a função do Scikit-Learn [`sklearn.model_selection.train_test_split()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)

#### Argumentos:

* matriz a ser dividida - `X` ou `y`
* `test_size` - `float` ou `int` do tamanho do dataset de teste (padrão $0.25$)
* `train_size` - padrão `1 - test_size`
* `random_state` - `int` - seed do gerador de número randômicos (replicabilidade)
"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X,
                                                    y,
                                                    test_size=0.25,
                                                    random_state=123)

print(f"Tamanho de X_train: {X_train.shape}")
print(f"Tamanho de X_test: {X_test.shape}")
print(f"Tamanho de y_train: {y_train.shape}")
print(f"Tamanho de y_test: {y_test.shape}")

"""### Regressão Linear
Usar a função do Scikit-Learn [`sklearn.linear_model.SGDRegressor()`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html)

#### Argumentos:
* `loss` - `str`
    * MSE - `'squared_loss'`
    * MAE - `'epsilon_insensitive'`
* `max_iter` - `int` - Número máximo de iterações do *Gradient Descent*
* `tol` - Tolerância - Critério de parada de treino
* `random_state` - `int` - seed do gerador de número randômicos (replicabilidade)
* `eta0` - `float` - Taxa de aprendizagem inicial
    * padrão `0.01`
* `learning_rate` - `str` - Taxa de aprendizagem
    * Constante - `'constant'`
    * Adaptativa - `'adaptive'`
* `n_iter_no_change` - `int` - Somente se usar Taxa de Aprendizagem Adaptativa

#### Retorna:
* Objeto `estimator` do Scikit-Learn
"""

from sklearn.linear_model import SGDRegressor

clf = SGDRegressor(loss='squared_loss',
                   learning_rate='constant',
                   max_iter=10,
                   eta0=0.01,
                   verbose=1,
                   tol=None,
                   random_state=123)

"""### Classe `Estimators`

* `.fit()` - Treina o Modelo
    * `X`
    * `y`
* `.predict()` - Gera predições do modelo
    * `X`
* `.coef_` - Retorna os coeficientes do modelo ($\theta_i$)
* `.intercept_` - Retorna o viés/constante (*bias/intercept*) do modelo ($\theta_0$)
"""

clf.fit(X_train, y_train)

clf.coef_.tolist()

# Coeficientes do modelo
for feature, coef in zip(boston['feature_names'].tolist(), clf.coef_.tolist()):
    print(f"{feature}: {int(coef)}")

# Constante do modelo
print(f"Constante: {int(clf.intercept_)}")

"""### Erro do Modelo

Erro do Modelo é $\pm \sqrt{MSE}$
"""

from sklearn.metrics import mean_absolute_error, mean_squared_error

y_pred = clf.predict(X_test)

print(f"Acurácia de Teste: ±{mean_absolute_error(y_test, y_pred):1.0f}")

"""### Efeito da Taxa de Aprendizagem $\eta$"""

import numpy as np
import matplotlib.pyplot as plt

clf = SGDRegressor()

# inicia duas listas vazias
mae_train_values = []
mae_test_values = []

# Eixo X do grafico
x_axis = np.linspace(1e-6, 1, 1000)

for i in x_axis:
    clf =  clf.set_params(eta0=i)
    clf.fit(X_train, y_train)
    
    y_train_pred = clf.predict(X_train)
    mae_train = mean_absolute_error(y_train, y_train_pred)
    mae_train_values.append(mae_train)
    
    y_test_pred = clf.predict(X_test)
    mae_test = mean_absolute_error(y_test, y_test_pred)
    mae_test_values.append(mae_test)
    
fig, axes = plt.subplots(2, 1, sharex=True)
axes[0].plot(x_axis, mae_train_values, color='orange')
axes[1].plot(x_axis, mae_test_values, color='blue')
axes[0].set_title('Erro de Treino')
axes[1].set_title('Erro de Teste')
axes[1].set_xlabel('Taxa de aprendizagem $\eta$')
axes[0].set_ylabel('$MAE$')
axes[1].set_ylabel('$MAE$')
fig.suptitle('Comparação Erro de Treino vs Erro de Teste')
plt.tight_layout()
plt.show()

"""### Efeito do Número de Iterações"""

# inicia duas listas vazias
mae_train_values = []
mae_test_values = []

# Eixo X do grafico
x_axis = np.linspace(10, 100000, 10)

for i in x_axis:
    clf.set_params(max_iter=i, eta0=1e-7, verbose=0)  # 0.0000007
    clf.fit(X_train, y_train)
    
    y_train_pred = clf.predict(X_train)
    mae_train = mean_absolute_error(y_train, y_train_pred)
    mae_train_values.append(mae_train)
    
    y_test_pred = clf.predict(X_test)
    mae_test = mean_absolute_error(y_test, y_test_pred)
    mae_test_values.append(mae_test)

fig, axes = plt.subplots(2, 1, sharex=True)
axes[0].plot(x_axis, mae_train_values, color='orange')
axes[1].plot(x_axis, mae_test_values, color='blue')
axes[0].set_title('Erro de Treino')
axes[1].set_title('Erro de Teste')
axes[1].set_xlabel('Número de Iterações')
axes[0].set_ylabel('$MAE$')
axes[1].set_ylabel('$MAE$')
fig.suptitle('Comparação Erro de Treino vs Erro de Teste')
plt.tight_layout()
plt.show()

"""## Atividade - Regressão com o dataset [Diabetes](https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset)

* $N = 442$
* Atributos: 10
    * `age`
    * `sex`
    * `bmi` Índice de Massa Corpórea (IMC) - *Body Mass Index* (BMI)
    * `bp` pressão arterial média *blood pressure* (bp)
    * `s1` colesterol total
    * `s2` colesterol LDL
    * `s3` colesterol HDL
    * `s4` colesterol VLDL
    * `s5` triglicerides
    * `s6` glicose
* Variável dependente: medida quantitativa de progressão da diabetes

* Achar o melhor `eta0` e os respectivos coeficientes dos atributos ($\theta_i$) e viés/constante ($\theta_0$)

>Obs: usar `test_size = 0.25` e `random_state = 123`
"""

from sklearn.datasets import load_diabetes

diabetes = load_diabetes()
X = diabetes['data']
y = diabetes['target']

print(f"Nomes dos Atributos: {diabetes['feature_names']}")
print(f"Tamanho de X: {X.shape}")
print(f"Tamanho de y: {y.shape}")

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=123)

print(f"Tamanho de X_train: {X_train.shape}")
print(f"Tamanho de X_test: {X_test.shape}")
print(f"Tamanho de y_train: {y_train.shape}")
print(f"Tamanho de y_test: {y_test.shape}")